#!/bin/bash

#SBATCH --account=stf218-arch
#SBATCH --partition=batch
#SBATCH --nodes=1
#SBATCH --cpus-per-task=288
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --time=00:11:00
#SBATCH --job-name=generate_tokenized
#SBATCH --output=generate_tokenized_%A_%a.out
#SBATCH --array=0

# activate venv
source /lustre/blizzard/stf218/scratch/emin/blizzardvenv/bin/activate

# set misc env vars
export LOGLEVEL=INFO
export OMP_NUM_THREADS=1
export HF_HOME="/lustre/blizzard/stf218/scratch/emin/huggingface"
export HF_DATASETS_CACHE="/lustre/blizzard/stf218/scratch/emin/huggingface"
export TRITON_CACHE_DIR="/lustre/blizzard/stf218/scratch/emin/triton"
export PYTORCH_KERNEL_CACHE_PATH="/lustre/blizzard/stf218/scratch/emin/pytorch_kernel_cache"
export MPLCONFIGDIR="/lustre/blizzard/stf218/scratch/emin/mplconfigdir"
export HF_HUB_OFFLINE=1
export GPUS_PER_NODE=4

# set network
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=3442

CONFIG_FILE=${CONFIG_FILE:-"./train_configs/primate_7B_8k_n_fixed_256_tokenizer_1x15_32k.toml"}

srun torchrun --nnodes $SLURM_NNODES --nproc_per_node 4 --max_restarts 1 --node_rank $SLURM_NODEID --rdzv_id 101 --rdzv_backend c10d --rdzv_endpoint "$MASTER_ADDR:$MASTER_PORT" ./generate_tokenized.py --config ${CONFIG_FILE} --tokenizer_path "tokenizers/tokenizer_primate_1x15_32k.pkl" --ckpt "outputs/primate_7B_8k_n_fixed_256_tokenizer_1x15_32k/checkpoint/step-99000"

echo "Done"